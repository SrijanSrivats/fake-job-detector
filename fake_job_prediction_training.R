# fake_job_prediction_full.R
# Complete, fixed version of Fake Job Prediction pipeline using XGBoost + caret
# Requirements: install packages listed below (run once)
# install.packages(c("tm","tidyverse","caret","randomForest","e1071","xgboost",
#                    "rpart","rpart.plot","pROC","ROSE","corrplot","ggplot2",
#                    "ggthemes","stringr","SnowballC","wordcloud","textmineR",
#                    "Matrix","kernlab"))

# Load libraries
library(tm)
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(xgboost)
library(rpart)
library(rpart.plot)
library(pROC)
library(ROSE)
library(corrplot)
library(ggplot2)
library(ggthemes)
library(stringr)
library(SnowballC)
library(wordcloud)
library(textmineR)
library(Matrix)
library(kernlab)

set.seed(123)

# Create output directories
dir.create("visualizations", showWarnings = FALSE)
dir.create("models", showWarnings = FALSE)

# -------------------------
# 1. LOAD DATA
# -------------------------
data_path <- "fake_job_postings.csv"
if (!file.exists(data_path)) stop(paste("Dataset not found at", data_path))
data <- read.csv(data_path, stringsAsFactors = FALSE, na.strings = c("", "NA"))

cat("Dataset Dimensions:", dim(data), "\n")
cat("Column Names:", paste(names(data), collapse = ", "), "\n")

# -------------------------
# 2. INITIAL CLEANING & MISSING HANDLING
# -------------------------
# Make sure column names are syntactically valid
names(data) <- make.names(names(data))

# Convert fraudulent to factor early (ensure it exists)
if (!"fraudulent" %in% names(data)) stop("Dataset must contain column named 'fraudulent'")
data$fraudulent <- as.integer(data$fraudulent) # ensure numeric 0/1
data$fraudulent[is.na(data$fraudulent)] <- 0
data$fraudulent <- factor(data$fraudulent, levels = c(0, 1), labels = c("No", "Yes"))

# Handle missing / blank text columns
text_cols <- c("title","location","department","company_profile","description",
               "requirements","benefits","employment_type","required_experience",
               "required_education","industry","function.")
for (col in intersect(text_cols, names(data))) {
  data[[col]][is.na(data[[col]])] <- ""
}

# Numeric binary fields: telecommuting, has_company_logo, has_questions
for (col in c("telecommuting","has_company_logo","has_questions")) {
  if (col %in% names(data)) {
    data[[col]] <- as.integer(replace(data[[col]], is.na(data[[col]]), 0))
  } else {
    data[[col]] <- 0L
  }
}

# Remove duplicates
data <- data[!duplicated(data), ]

# -------------------------
# 3. FEATURE ENGINEERING
# -------------------------
# Add simple text-length features and keyword counts
data <- data %>%
  mutate(
    desc_length = nchar(description),
    req_length = nchar(requirements),
    benefits_length = nchar(benefits),
    company_profile_length = nchar(company_profile),
    title_length = nchar(title),
    # basic keyword signals
    salary_keywords = str_count(tolower(paste(description, requirements, benefits, sep = " ")),
                               "unpaid|volunteer|no salary|commission only|paid after"),
    urgency_keywords = str_count(tolower(paste(description, requirements, sep = " ")),
                                 "immediate|urgent|quick|start now|apply now|ASAP")
  )

# winsorize function for numeric columns
handle_outliers <- function(x) {
  if (!is.numeric(x)) return(x)
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  x[x < lower_bound] <- lower_bound
  x[x > upper_bound] <- upper_bound
  return(x)
}

numeric_cols_to_trim <- c("desc_length","req_length","benefits_length","company_profile_length","title_length")
for (col in numeric_cols_to_trim) {
  if (col %in% names(data)) data[[col]] <- handle_outliers(data[[col]])
}

# Text preprocessing helper
preprocess_text <- function(text) {
  if (is.na(text) || text == "") return("")
  text <- tolower(text)
  text <- removePunctuation(text)
  text <- removeNumbers(text)
  text <- removeWords(text, stopwords("english"))
  text <- stripWhitespace(text)
  text <- trimws(text)
  return(text)
}

cat("Preprocessing text columns (this may take a moment)...\n")
data$description_clean <- vapply(data$description, preprocess_text, FUN.VALUE = "")
data$requirements_clean <- vapply(data$requirements, preprocess_text, FUN.VALUE = "")

# Function to create simple text-derived numeric features
create_text_features <- function(texts, prefix) {
  fake_keywords <- c("money","cash","quick","easy","guarantee","free",
                     "profit","income","rich","wealth","million","billion",
                     "immediate","urgent","commission")
  
  # Each vapply call should return a single integer per row → use integer(1)
  word_count <- vapply(texts, function(x) {
    if (is.na(x) || x == "") return(0L)
    w <- unlist(strsplit(x, "\\s+"))
    w <- w[w != ""]
    length(w)
  }, integer(1))
  
  char_count <- nchar(texts)
  avg_word_length <- ifelse(word_count > 0, char_count / word_count, 0)
  
  keyword_counts <- vapply(texts, function(txt) {
    if (is.na(txt) || txt == "") return(0L)
    sum(vapply(fake_keywords, function(k) str_count(txt, fixed(k)), integer(1)))
  }, integer(1))
  
  df <- data.frame(
    word_count = word_count,
    char_count = char_count,
    avg_word_length = avg_word_length,
    keyword_counts = keyword_counts
  )
  colnames(df) <- paste0(prefix, c("_word_count", "_char_count", "_avg_word_length", "_fake_keywords"))
  return(df)
}

desc_feats <- create_text_features(data$description_clean, "desc")
req_feats <- create_text_features(data$requirements_clean, "req")

# Combine features
essential_features <- data %>%
  transmute(
    telecommuting = telecommuting,
    has_company_logo = has_company_logo,
    has_questions = has_questions,
    desc_length = desc_length,
    req_length = req_length,
    benefits_length = benefits_length,
    company_profile_length = company_profile_length,
    title_length = title_length,
    salary_keywords = salary_keywords,
    urgency_keywords = urgency_keywords
  )

final_features <- cbind(essential_features, desc_feats, req_feats)
final_data <- as.data.frame(final_features)
final_data$fraudulent <- data$fraudulent  # ensure target exists in final_data

# Remove near-zero variance features if any
nzv <- nearZeroVar(final_data[, setdiff(names(final_data), "fraudulent")], saveMetrics = FALSE)
if (length(nzv) > 0) {
  warning("Removing near-zero variance columns: ", paste(names(final_data)[nzv], collapse = ", "))
  final_data <- final_data[, setdiff(names(final_data), names(final_data)[nzv]), drop = FALSE]
}

# -------------------------
# 4. DATA QUALITY CHECK
# -------------------------
check_data_quality <- function(df, name) {
  cat("\n-- Data Quality Check:", name, "--\n")
  cat("Dimensions:", dim(df), "\n")
  cat("NA count:", sum(is.na(df)), "\n")
  cat("Target distribution:\n")
  print(table(df$fraudulent))
  print(round(prop.table(table(df$fraudulent)), 4))
}
check_data_quality(final_data, "Final features (before sampling)")

# -------------------------
# 5. HANDLE CLASS IMBALANCE (ROSE)
# -------------------------
cat("\nApplying ROSE to balance classes...\n")
# ovun.sample wants formula; ensure fraudulent exists as factor in final_data
final_data$fraudulent <- factor(final_data$fraudulent, levels = c("No","Yes"))

balanced_obj <- ovun.sample(fraudulent ~ ., data = final_data, method = "both", p = 0.5, seed = 123)
final_data_balanced <- balanced_obj$data
check_data_quality(final_data_balanced, "After ROSE balancing")

# -------------------------
# 6. VISUALIZATIONS (basic)
# -------------------------
p1 <- ggplot(final_data_balanced, aes(x = fraudulent, fill = fraudulent)) +
  geom_bar() + labs(title = "Class Distribution After ROSE", x = "Fraudulent", y = "Count") +
  theme_minimal()
ggsave("visualizations/class_distribution.png", p1, width = 7, height = 5)

# -------------------------
# 7. TRAIN/TEST SPLIT and SCALING
# -------------------------
set.seed(123)
train_index <- createDataPartition(final_data_balanced$fraudulent, p = 0.8, list = FALSE)
train_data <- final_data_balanced[train_index, , drop = FALSE]
test_data <- final_data_balanced[-train_index, , drop = FALSE]

cat("Train dimensions:", dim(train_data), "Test dimensions:", dim(test_data), "\n")

# Preprocess numeric predictors (center & scale) - exclude target
predictor_names <- setdiff(colnames(train_data), "fraudulent")
preproc <- preProcess(train_data[, predictor_names], method = c("center", "scale"))
train_scaled <- train_data
train_scaled[, predictor_names] <- predict(preproc, train_data[, predictor_names])
test_scaled <- test_data
test_scaled[, predictor_names] <- predict(preproc, test_data[, predictor_names])

# Ensure factors are correct for caret
train_scaled$fraudulent <- factor(train_scaled$fraudulent, levels = c("No", "Yes"))
test_scaled$fraudulent <- factor(test_scaled$fraudulent, levels = c("No", "Yes"))

# -------------------------
# 8. MODEL TRAINING (XGBoost via caret)
# -------------------------
train_control <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  verboseIter = TRUE,
  allowParallel = TRUE
)

# A small grid
xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

cat("Training XGBoost model (may take a minute)...\n")
set.seed(123)
xgb_model <- train(
  x = train_scaled[, predictor_names],
  y = train_scaled$fraudulent,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = xgb_grid,
  metric = "ROC",
  verbosity = 0
)

# Save the model
saveRDS(xgb_model, file = "models/xgboost_model.rds")
cat("Model saved to models/xgboost_model.rds\n")

# -------------------------
# 9. EVALUATION
# -------------------------
preds <- predict(xgb_model, test_scaled[, predictor_names])
probs <- predict(xgb_model, test_scaled[, predictor_names], type = "prob")

conf <- confusionMatrix(preds, test_scaled$fraudulent, positive = "Yes")
accuracy <- conf$overall["Accuracy"]
precision <- conf$byClass["Precision"]
recall <- conf$byClass["Recall"]
f1 <- conf$byClass["F1"]

# AUC
roc_obj <- roc(response = ifelse(test_scaled$fraudulent == "Yes", 1, 0), predictor = probs[, "Yes"])
auc_value <- as.numeric(auc(roc_obj))

cat("\n=== MODEL PERFORMANCE ===\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("Recall:", round(recall, 4), "\n")
cat("F1:", round(f1, 4), "\n")
cat("AUC:", round(auc_value, 4), "\n")
cat("Confusion Matrix:\n")
print(conf$table)

# Save performance table
results_summary <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score", "AUC"),
  Value = c(round(as.numeric(accuracy), 4),
            round(as.numeric(precision), 4),
            round(as.numeric(recall), 4),
            round(as.numeric(f1), 4),
            round(auc_value, 4))
)
write.csv(results_summary, "visualizations/model_performance.csv", row.names = FALSE)
print(results_summary)

# ROC plot saved
png("visualizations/roc_curve.png", width = 800, height = 600)
plot(roc_obj, main = paste0("ROC Curve - XGBoost (AUC=", round(auc_value, 4), ")"))
dev.off()

# Feature importance from final model
if (!is.null(xgb_model$finalModel)) {
  importance_matrix <- xgb.importance(feature_names = predictor_names, model = xgb_model$finalModel)
  if (nrow(importance_matrix) > 0) {
    topN <- min(10, nrow(importance_matrix))
    top_features <- importance_matrix[1:topN, ]
    p_imp <- ggplot(top_features, aes(x=reorder(Feature, Gain), y=Gain)) +
      geom_col() + coord_flip() + labs(title = "XGBoost Feature Importance (Gain)", x = "Feature", y = "Gain") +
      theme_minimal()
    ggsave("visualizations/xgb_feature_importance.png", p_imp, width = 9, height = 6)
  }
}

cat("\n=== PIPELINE COMPLETED SUCCESSFULLY ===\n")
cat("Visualizations saved in 'visualizations/' and model saved in 'models/'.\n")

# ===============================================
# Fake Job Prediction - Model Loading and Inference (with Probabilities)
# ===============================================

# Required packages
library(tm)
library(stringr)
library(caret)
library(xgboost)

# Helper operator for defaults
`%||%` <- function(a, b) if (!is.null(a)) a else b

# -------------------------
# 1. LOAD TRAINED MODEL
# -------------------------
model_path <- "models/xgboost_model.rds"
if (!file.exists(model_path)) stop("❌ Model file not found. Make sure models/xgboost_model.rds exists.")
xgb_model <- readRDS(model_path)
cat("✅ Model loaded successfully!\n")

# -------------------------
# 2. TEXT PREPROCESSING FUNCTION (same as training)
# -------------------------
preprocess_text <- function(text) {
  if (is.na(text) || text == "") return("")
  text <- tolower(text)
  text <- removePunctuation(text)
  text <- removeNumbers(text)
  text <- removeWords(text, stopwords("english"))
  text <- stripWhitespace(text)
  text <- trimws(text)
  return(text)
}

# Create text-derived numeric features
create_text_features <- function(texts, prefix) {
  fake_keywords <- c("money","cash","quick","easy","guarantee","free",
                     "profit","income","rich","wealth","million","billion",
                     "immediate","urgent","commission")
  
  word_count <- vapply(texts, function(x) {
    if (is.na(x) || x == "") return(0L)
    w <- unlist(strsplit(x, "\\s+"))
    w <- w[w != ""]
    length(w)
  }, integer(1))
  
  char_count <- nchar(texts)
  avg_word_length <- ifelse(word_count > 0, char_count / word_count, 0)
  
  keyword_counts <- vapply(texts, function(txt) {
    if (is.na(txt) || txt == "") return(0L)
    sum(vapply(fake_keywords, function(k) str_count(txt, fixed(k)), integer(1)))
  }, integer(1))
  
  df <- data.frame(
    word_count = word_count,
    char_count = char_count,
    avg_word_length = avg_word_length,
    keyword_counts = keyword_counts
  )
  colnames(df) <- paste0(prefix, c("_word_count", "_char_count", "_avg_word_length", "_fake_keywords"))
  return(df)
}

# -------------------------
# 3. PREPARE FEATURES FOR PREDICTION
# -------------------------
prepare_features <- function(job_post) {
  # Ensure text fields exist
  text_fields <- c("title","description","requirements","benefits","company_profile")
  for (f in text_fields) {
    if (!f %in% names(job_post)) job_post[[f]] <- ""
    if (is.na(job_post[[f]])) job_post[[f]] <- ""
  }
  
  # Numeric flags
  telecommuting <- as.integer(job_post$telecommuting %||% 0)
  has_company_logo <- as.integer(job_post$has_company_logo %||% 0)
  has_questions <- as.integer(job_post$has_questions %||% 0)
  
  # Preprocess text
  desc_clean <- preprocess_text(job_post$description)
  req_clean <- preprocess_text(job_post$requirements)
  
  # Feature engineering
  desc_feats <- create_text_features(desc_clean, "desc")
  req_feats <- create_text_features(req_clean, "req")
  
  # Basic numeric features
  essential <- data.frame(
    telecommuting = telecommuting,
    has_company_logo = has_company_logo,
    has_questions = has_questions,
    desc_length = nchar(job_post$description),
    req_length = nchar(job_post$requirements),
    benefits_length = nchar(job_post$benefits),
    company_profile_length = nchar(job_post$company_profile),
    title_length = nchar(job_post$title),
    salary_keywords = str_count(tolower(paste(job_post$description, job_post$requirements, job_post$benefits)), 
                                "unpaid|volunteer|no salary|commission only|paid after"),
    urgency_keywords = str_count(tolower(paste(job_post$description, job_post$requirements)), 
                                 "immediate|urgent|quick|start now|apply now|asap")
  )
  
  final <- cbind(essential, desc_feats, req_feats)
  return(final)
}

# -------------------------
# 4. PREDICTION FUNCTION (with probabilities)
# -------------------------
predict_fake_job <- function(job_post) {
  features <- prepare_features(job_post)
  
  # Predict probabilities using the model
  probs <- predict(xgb_model, newdata = features, type = "prob")
  
  # If output is numeric (some caret versions)
  if (is.numeric(probs)) {
    # When the model is not wrapped with caret's twoClassSummary
    probs <- data.frame(No = 1 - probs, Yes = probs)
  }
  
  # Determine class with highest probability
  prediction <- ifelse(probs$Yes >= 0.5, "Yes", "No")
  
  result <- data.frame(
    Prediction = prediction,
    Probability_Fake = round(probs$Yes, 4),
    Probability_Genuine = round(probs$No, 4),
    Message = ifelse(prediction == "Yes", 
                     "⚠️ This job post is likely FAKE.", 
                     "✅ This job post seems GENUINE.")
  )
  return(result)
}

# -------------------------
# 5. EXAMPLE USAGE
# -------------------------
example_job <- list(
  title = "Work from home data entry - earn Rs.5000 daily",
  description = "Urgent hiring! Immediate start. Earn money fast with simple typing work. No experience needed.",
  requirements = "Computer, internet connection. Apply now!",
  benefits = "Quick payments and flexible schedule.",
  company_profile = "We are an online freelancing company.",
  telecommuting = 1,
  has_company_logo = 0,
  has_questions = 0
)

cat("\n🔍 Predicting for example job post...\n")
result <- predict_fake_job(example_job)
print(result)